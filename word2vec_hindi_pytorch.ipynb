{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import numpy as np\n",
    "import torch.functional as F\n",
    "import torch.nn.functional as F\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Embedding:\n",
    "    \n",
    "    def __init__(self, lr, emb_size):\n",
    "        self.emb_size = emb_size\n",
    "        self.lr = lr\n",
    "        \n",
    "    def initialise(self, voc_size, device):\n",
    "        self.w1 = torch.randn((self.emb_size, voc_size), device=device, requires_grad=True)\n",
    "        self.w2 = torch.randn((voc_size, self.emb_size), device=device, requires_grad=True)\n",
    "    \n",
    "    @staticmethod\n",
    "    def generate_inp_vector(word_idx, voc_size):\n",
    "        print(\"vocabulary size : \",voc_size)\n",
    "        x = np.zeros(voc_size)\n",
    "        print(x.shape)\n",
    "        x[word_idx] = 1\n",
    "        return torch.tensor(x)\n",
    "    \n",
    "    def training(self, epochs, trg_pairs):\n",
    "        '''\n",
    "        trg_pairs : list of lists of pairs\n",
    "    \n",
    "        '''\n",
    "        loss_val = 0\n",
    "        for i in range(epochs):\n",
    "            for pair in trg_pairs:\n",
    "                center, context = pair[0], pair[1]\n",
    "                print(center, context)\n",
    "                x = Model.generate_inp_vector(center, self.voc_size)\n",
    "                y_true = torch.from_numpy(np.array([context]))\n",
    "                \n",
    "                h = torch.matmul(self.w1, x)\n",
    "                o = torch.matmul(self.w2, h)\n",
    "                \n",
    "                y_pred = F.log_softmax(0, dim=0)\n",
    "                loss = F.nll_loss(y_pred.view(1,-1), y_true)\n",
    "                \n",
    "                loss_val += loss.data.item()\n",
    "                loss.backward()\n",
    "                \n",
    "                self.w1 -= self.lr * self.w1.grad.data\n",
    "                self.w2 -= self.lr * self.w2.grad.data\n",
    "            \n",
    "            print(\"loss after {} : {}\".format(i,loss_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1843735\n",
      "85427\n"
     ]
    }
   ],
   "source": [
    "with open('words_in_vocab.txt', 'w') as vocab:\n",
    "    vocab_size = data.readline()\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocabulary size : 12557, truncated corpus size : 1575620\n"
     ]
    }
   ],
   "source": [
    "lr = 0.001\n",
    "emb_size = 50\n",
    "\n",
    "emb = Embedding(lr, emb_size)\n",
    "emb.initialise(vocab_size, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "trg_pairs = []\n",
    "with open('training_data.txt', 'r') as data\n",
    "    for sample in data:\n",
    "        center, context = sample.strip().split('\\t')\n",
    "        center, context = int(center), int(context)\n",
    "        trg_pairs.append([center,context])\n",
    "    vocab_size = vocab.readline()\n",
    "    print(vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2vmodel.initialise()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 2\n",
      "vocabulary size :  12557\n",
      "(12557,)\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "matmul(): argument 'other' (position 2) must be Tensor, not numpy.ndarray",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-50-5605a7c172c0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mw2vmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrg_pairs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-45-92cd62e1ba0f>\u001b[0m in \u001b[0;36mtraining\u001b[0;34m(self, epochs, trg_pairs)\u001b[0m\n\u001b[1;32m     91\u001b[0m                 \u001b[0my_true\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 93\u001b[0;31m                 \u001b[0mh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mw1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     94\u001b[0m                 \u001b[0mo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mw2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: matmul(): argument 'other' (position 2) must be Tensor, not numpy.ndarray"
     ]
    }
   ],
   "source": [
    "w2vmodel.training(2, trg_pairs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
